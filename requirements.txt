# ============================================
# Svarog LLM Training Dependencies
# Target: RTX 2080 Super (8GB VRAM) + i9 10900F
# CUDA: 12.1 (upgradeable to 12.4)
# Author: MagistrTheOne | Krasnodar 2025
# ============================================

# === Core ML Stack (CUDA 12.1) ===
torch==2.4.1+cu121             # Core tensor backend (supports AMP, SDPA)
torchvision==0.19.1+cu121
torchaudio==2.4.1+cu121
nvidia-cublas-cu12==12.4.5.8   # (safety: ensure CUDA 12.4 kernels)
nvidia-cudnn-cu12==9.1.0.70    # Critical for transformer attention speed

# === Transformer & Tokenization ===
transformers==4.44.2           # Stable w/ PyTorch 2.4
tokenizers==0.19.1             # Fast Rust backend
sentencepiece==0.2.0
safetensors==0.4.4             # Faster + safer serialization

# === Dataset & Streaming ===
datasets==2.21.0
pyarrow==16.1.0
fsspec==2024.6.0
aiohttp==3.9.5
huggingface-hub==0.24.6        # HF dataset sync + push

# === Training Optimization ===
deepspeed==0.14.4              # ZeRO stage-2 support
accelerate==0.33.0             # Seamless distributed training
bitsandbytes==0.44.1           # 8-bit optimizers (VRAM saver)
ninja==1.11.1.1
xformers==0.0.27.post2         # Flash attention kernel for PyTorch 2.4

# === Text Cleaning / Preproc ===
ftfy==6.2.0
langdetect==1.0.9
regex==2024.7.24
unidecode==1.3.8               # Converts non-UTF chars (esp. Cyrillic oddities)

# === Monitoring & Tracking ===
wandb==0.18.1
mlflow==2.15.1
tensorboard==2.17.0
py3nvml==0.2.7
gpustat==1.1.1
rich==13.9.1                   # Pretty console output for logs

# === Utilities ===
numpy==1.26.4
pandas==2.2.3
tqdm==4.66.5
psutil==6.0.0
pyyaml==6.0.2
packaging==24.1
